12.3 TELJES MODELL b37cebd790c0188797b1211d340b2f4d48ccddeb
!pip install transformers torch pandas gradio

import pandas as pd
import os
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
import torch
import gradio as gr





# Adathalmaz betöltése a Hugging Face-ről
df = pd.read_csv("hf://datasets/gofilipa/bedtime_stories/stories.csv")

# Ellenőrizzük az oszlopneveket
print(df.columns)





# Szövegek kinyerése a 'stories' oszlopból
stories = df['stories'].tolist()

print(stories[:1])  # Az első mese kiíratása






# Modell és tokenizer betöltése
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Eszközbeállítás
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Adatok tokenizálása
inputs = tokenizer(stories, return_tensors='pt', max_length=512, truncation=True, padding=True)

# Dataset létrehozása
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = item['input_ids'].clone()
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

dataset = CustomDataset(inputs)











# Finomhangolási beállítások
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=2,
    warmup_steps=200,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=50,
)

# Trainer inicializálása
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

# Modell betanítása
trainer.train()












#1.  Alap promptal

# Történetgeneráló funkció a finomhangolt nyelvezettel és stílusban
def generate_story(protagonist, location, brief_event, paragraphs, words_per_paragraph):
    # Alap prompt a megadott paraméterekkel, a finomhangolt nyelvezet alapján
    initial_prompt = (f"In the {location}, a brave explorer named {protagonist} found themselves on an unusual adventure. "
                      f"One day, {protagonist} discovered {brief_event}. This discovery would lead to wonders and challenges beyond their wildest dreams.")

    story = ""
    prompt = initial_prompt  # Start with the initial prompt for the first paragraph

    for i in range(paragraphs):
        # Generate input_ids with attention mask and set pad token id for reliable generation
        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
        attention_mask = torch.ones(input_ids.shape, device=device)

        output = model.generate(
            input_ids,
            max_length=words_per_paragraph + len(input_ids[0]),
            num_return_sequences=1,
            no_repeat_ngram_size=2,
            temperature=0.7,  # Reduced temperature for more controlled output
            top_k=30,  # Lowered top_k for more focused choices
            top_p=0.9,
            do_sample=True,
            attention_mask=attention_mask,
            pad_token_id=tokenizer.eos_token_id
        )

        paragraph = tokenizer.decode(output[0], skip_special_tokens=True)

        # Add the generated paragraph to the story, removing any repeated prompt text
        story += "\n\n" + paragraph[len(prompt):]

        # Reset the prompt to keep reinforcing the initial setup for each new paragraph
        prompt = (f"As {protagonist} continued their adventure in the {location}, "
                  f"they thought back to the moment they discovered {brief_event}. Each step was filled with new mysteries and friends along the way.")

    return story.strip()

# Teszt mese generálása a megadott paraméterekkel
test_story = generate_story(
    protagonist="Adam",
    location="enchanted forest",
    brief_event="a glowing, magical animal",
    paragraphs=3,
    words_per_paragraph=100
)

print("Generated Test Story:")
print(test_story)


















# 0.4 VERZIÓ
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Modell és tokenizer betöltése 
# tokenizer = GPT2Tokenizer.from_pretrained("your-fine-tuned-model-path")
# model = GPT2LMHeadModel.from_pretrained("your-fine-tuned-model-path")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Minimális paraméterekkel működő történetgeneráló funkció
def test_fine_tuned_model_simple(protagonist="A brave adventurer Adam", max_length=300):
    # Egyszerű, mesés bevezető prompt
    intro_prompt = f"Once upon a time, {protagonist} went on an incredible journey."

    # Prompt tokenizálása és átküldése a modellnek
    input_ids = tokenizer.encode(intro_prompt, return_tensors='pt').to(device)

    # Generálás a modell segítségével
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=0.6,  # kreatívitás
        top_k=90,         # irányítottabb generálásért
        top_p=0.95,       # összpontosított történetért
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    # Kimenet dekódolása és visszaadása
    story = tokenizer.decode(output[0], skip_special_tokens=True)
    return story

# Teszt futtatása egy alapértelmezett főszereplővel
generated_story = test_fine_tuned_model_simple()

print("Generated Story:")
print(generated_story)
