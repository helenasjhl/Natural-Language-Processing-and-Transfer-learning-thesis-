# üìå **Saj√°t Nyelvi Modell**

Ez a repository a **Saj√°t Nyelvi Modell** fejleszt√©s√©re √©s alkalmaz√°s√°ra szolg√°l, amely egy GPT-2 alap√∫ mese gener√°l√≥ modell. A c√©l egy term√©szetes nyelvi feldolgoz√°son (NLP) alapul√≥, transzfer tanul√°ssal finomhangolt AI l√©trehoz√°sa.

## üìñ **Bevezet√©s**&#x20;

A term√©szetes nyelvi feldolgoz√°s (NLP) egyik legfontosabb alkalmaz√°si ter√ºlete a sz√∂vegalkot√°s. Az ut√≥bbi √©vekben a nagy nyelvi modellek (LLM) elterjed√©se lehet≈ëv√© tette a mesters√©ges intelligencia sz√°m√°ra, hogy koherens, emberi nyelven √≠rt sz√∂vegeket gener√°ljon. Ez a projekt egy GPT-2 alap√∫ mese gener√°l√≥ modellt val√≥s√≠t meg, amely gyermekbar√°t t√∂rt√©neteket k√©pes el≈ë√°ll√≠tani.

## üìÇ **Tartalom**

- **sajat\_model.txt** - A saj√°t fejleszt√©s≈± modell teljes k√≥dja
- **szakdolgozat.pdf** - A szakdolgozat, amely bemutatja a modell fejleszt√©si folyamat√°t
- **README.md** - Ez a dokumentum
- **k√≥dlista_szakdolgozat** - A szakdolgozatomban l√©v≈ë √∂sszes k√≥dot tartalmazza, nem csak a saj√°t modellt

## üõ†Ô∏è **Haszn√°lt technol√≥gi√°k**

- **Python** (3.x)
- **Hugging Face Transformers** (GPT-2)
- **Torch** (PyTorch)
- **Pandas** (adatkezel√©s)
- **Gradio** (interakt√≠v bemutat√≥ fel√ºlet)

## üß† **Modell fel√©p√≠t√©se**

A modell egy el≈ëre betan√≠tott **GPT-2** nyelvi modell finomhangol√°s√°n alapul. A finomhangol√°shoz a **Hugging Face Datasets** k√∂nyvt√°ron kereszt√ºl el√©rhet≈ë **'gofilipa/bedtime\_stories'** datasetet haszn√°ltam, amely gyermekek sz√°m√°ra k√©sz√ºlt esti mes√©ket tartalmaz. Ennek c√©lja, hogy a gener√°lt sz√∂vegek term√©szetesebbek √©s √©lvezhet≈ëbbek legyenek a fiatalabb k√∂z√∂ns√©g sz√°m√°ra. Ennek c√©lja, hogy a gener√°lt sz√∂vegek v√°ltozatosak √©s kultur√°lisan gazdagok legyenek.
A modell egy el≈ëre betan√≠tott **GPT-2** nyelvi modell finomhangol√°s√°n alapul. A finomhangol√°s sor√°n mese adatb√°zisokat haszn√°ltunk, hogy a gener√°lt sz√∂vegek a c√©lk√∂z√∂ns√©g sz√°m√°ra megfelel≈ë st√≠lusban √©s tartalommal k√©sz√ºljenek el.

## üìä **Modell betan√≠t√°sa**

A betan√≠t√°shoz a **Hugging Face Transformers** √©s **PyTorch** k√∂nyvt√°rakat haszn√°ltuk. A finomhangol√°s folyamata:

1. Adatok el≈ëk√©sz√≠t√©se √©s tokeniz√°l√°sa
2. Modell finomhangol√°sa el≈ëre betan√≠tott GPT-2 seg√≠ts√©g√©vel
3. Modell teljes√≠tm√©ny√©nek √©rt√©kel√©se
4. Gener√°lt sz√∂vegek tesztel√©se √©s ki√©rt√©kel√©se

A modell √∫jratan√≠t√°s√°hoz:

```python
trainer.train()
```

## üöÄ **Hogyan haszn√°ld?**

### 1. Telep√≠tsd a sz√ºks√©ges csomagokat:

```bash
pip install transformers torch pandas gradio
```

### 2. Futtasd a modellt:

A saj√°t modell bet√∂lt√©s√©hez √©s haszn√°lat√°hoz:

```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# Modell √©s tokenizer bet√∂lt√©se
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2").to("cuda" if torch.cuda.is_available() else "cpu")

# Teszt gener√°l√°s
prompt = "Egyszer volt, hol nem volt"
input_ids = tokenizer.encode(prompt, return_tensors='pt')
output = model.generate(input_ids, max_length=100, num_return_sequences=1)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

## üîç **Eredm√©nyek √©s j√∂v≈ëbeli fejleszt√©sek**

A modell sikeresen gener√°l mes√©ket adott kulcsszavak alapj√°n.

## ‚úÖ **K√∂vetkeztet√©s**

A jelenlegi projekt egy hat√©kony m√≥dszert k√≠n√°l arra, hogy a mesters√©ges intelligencia mese gener√°l√°sra legyen k√©pes. A transzfer tanul√°s r√©v√©n el√©rt finomhangol√°s lehet≈ës√©get biztos√≠t a modell tov√°bbfejleszt√©s√©re √©s sz√©lesebb k√∂r≈± alkalmaz√°s√°ra. 

